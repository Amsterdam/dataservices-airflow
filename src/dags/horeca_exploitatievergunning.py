from contextlib import closing
from typing import Final, Optional

import pandas as pd
from airflow import DAG
from airflow.operators.python import PythonOperator
from postgres_on_azure_operator import PostgresOnAzureOperator
from common import default_args
from common.db import DatabaseEngine, get_ora_engine, wkt_loads_wrapped
from common.sql import SQL_CHECK_COUNT, SQL_CHECK_GEO
from contact_point.callbacks import get_contact_point_on_failure_callback
from geoalchemy2 import Geometry
from postgres_check_operator import PostgresCheckOperator
from postgres_permissions_operator import PostgresPermissionsOperator
from psycopg2 import sql
from sqlalchemy.types import Date, BigInteger, Text

dag_id = "horeca_exploitatievergunning"

table_name = dag_id
table_name_new = f"{table_name}_new"
SQL_TABLE_RENAME: Final = f"""
    DROP TABLE IF EXISTS {table_name} CASCADE;
    ALTER TABLE {table_name_new} RENAME TO {table_name};
    ALTER TABLE {table_name} RENAME CONSTRAINT {table_name_new}_pkey TO {table_name}_pkey;
    ALTER INDEX ix_{table_name_new}_id RENAME TO ix_{table_name}_id;
    ALTER INDEX idx_{table_name_new}_locatie RENAME TO idx_{table_name}_locatie;
    ALTER INDEX idx_{table_name_new}_terrasgeometrie RENAME TO idx_{table_name}_terrasgeometrie;
"""


def load_from_dwh(table_name: str, source_srid: int, dataset_name:Optional[str]=None, **context) -> None:
    """Imports data from source into target database.

    Args:
        table_name: Name of target table to import data.
        source_srid: SRID of source.
        dataset_name: Name of the dataset as known in the Amsterdam schema.
            Since the DAG name can be different from the dataset name, the latter
            can be explicity given. Only applicable for Azure referentie db connection.
            Defaults to None. If None, it will use the execution context to get the
            DAG id as surrogate. Assuming that the DAG id equals the dataset name
            as defined in Amsterdam schema.

    Executes:
        SQL statements
    """
    context = context['dag'].dag_id
    postgreshook_instance = DatabaseEngine(dataset_name=dataset_name, context=context).get_postgreshook_instance()
    db_engine = DatabaseEngine(dataset_name=dataset_name, context=context).get_engine()
    dwh_ora_engine = get_ora_engine("oracle_dwh_stadsdelen")
    with dwh_ora_engine.get_conn() as connection:
        df = pd.read_sql(
            """
            select ZAAKNUMMER
                , ZAAKNAAM
                , ADRES
                , ZAAK_CATEGORIE
                , ZAAK_SPECIFICATIE
                , BEGINDATUM
                , EINDDATUM
                , OPENINGSTIJDEN_ZO_DO_VAN
                , OPENINGSTIJDEN_ZO_DO_TOT
                , OPENINGSTIJDEN_VR_ZA_VAN
                , OPENINGSTIJDEN_VR_ZA_TOT
                , O_TIJDEN_TERRAS_ZO_DO_VAN
                , O_TIJDEN_TERRAS_ZO_DO_TOT
                , O_TIJDEN_TERRAS_VR_ZA_VAN
                , O_TIJDEN_TERRAS_VR_ZA_TOT
                , LOCATIE_WKT AS LOCATIE
                , TERRASGEOMETRIE_WKT AS TERRASGEOMETRIE
                , POSTCODE
                , STATUS_VERGUNNING
                , STATUS_TIJDELIJK_TERRAS
                , TOESTEMMING_TIJDELIJK_TERRAS
                , PUBL_BESLUIT_TIJDELIJK_TERRAS
                , TIJDELIJK_TERRAS_DETAILS
                , STATUS_VERLENG_TIJDELK_TERRAS
                , VERLENG_TIJDELK_TERRAS_DETAIL
            from DMDATA.HORECA_EXPLOITATIEVERGUNNING_V
        """,
            connection,
            coerce_float=True,
            params=None,
            parse_dates=["begindatum", "einddatum"],
            columns=None,
            chunksize=None,
        )
        # it seems that get_conn() makes the columns case sensitive
        # lowercase all columns so the database will handle them as case insensitive
        df.columns = map(str.lower, df.columns)
        df["locatie"] = df["locatie"].apply(func=wkt_loads_wrapped, source_srid=source_srid, geom_type_family='point')
        df["terrasgeometrie"] = df["terrasgeometrie"].apply(
            func=wkt_loads_wrapped, source_srid=source_srid, geom_type_family='polygon'
        )
        dtype = {
            "zaaknummer": BigInteger(),
            "zaaknaam": Text(),
            "zaakcategorie": Text(),
            "zaakspecificatie": Text(),
            "begindatum": Date(),
            "einddatum": Date(),
            "openingstijden_zo_do_van": Text(),
            "openingstijden_zo_do_tot": Text(),
            "openingstijden_vr_za_van": Text(),
            "openingstijden_vr_za_tot": Text(),
            "o_tijden_terras_zo_do_van": Text(),
            "o_tijden_terras_zo_do_tot": Text(),
            "o_tijden_terras_vr_za_van": Text(),
            "o_tijden_terras_vr_za_tot": Text(),
            "locatie": Geometry(geometry_type="POINT", srid=source_srid),
            "terrasgeometrie": Geometry(geometry_type="MULTIPOLYGON", srid=source_srid),
            "postcode": Text(),
            "status_vergunning": Text(),
            "status_tijdelijk_terras": Text(),
            "toestemming_tijdelijk_terras": Text(),
            "publ_besluit_tijdelijk_terras": Text(),
            "tijdelijk_terras_details": Text(),
            "status_verleng_tijdelk_terras": Text(),
            "verleng_tijdelk_terras_detail": Text(),
        }
        df.to_sql(table_name, db_engine, if_exists="replace", index_label="id", dtype=dtype)

        with closing(postgreshook_instance.get_conn().cursor()) as cur:
            cur.execute(
                sql.SQL("ALTER TABLE {table_name} ADD PRIMARY KEY (ID); COMMIT;").format(
                    table_name=sql.Identifier(table_name)
                )
            )
            if source_srid != 28992:
                cur.execute(
                    sql.SQL(
                        """ALTER TABLE {table_name}
                        ALTER COLUMN terrasgeometrie TYPE geometry(MultiPolygon,28992)
                        USING ST_Transform(terrasgeometrie,28992),
                        ALTER COLUMN locatie TYPE geometry(Point,28992)
                        USING ST_Transform(locatie,28992); COMMIT;"""
                    ).format(table_name=sql.Identifier(table_name))
                )
            cur.execute(
                sql.SQL(
                    """UPDATE {table_name}
                 SET terrasgeometrie = ST_CollectionExtract(ST_Makevalid(terrasgeometrie), 3)
                 WHERE ST_IsValid(terrasgeometrie) = False; COMMIT;"""
                ).format(table_name=sql.Identifier(table_name))
            )
            cur.execute(
                sql.SQL(
                    """ALTER TABLE {table_name} RENAME COLUMN VERLENG_TIJDELK_TERRAS_DETAIL
                    to VERLENGING_TIJDELIJK_TERRAS_DETAILS;
                 ALTER TABLE {table_name} RENAME COLUMN STATUS_VERLENG_TIJDELK_TERRAS
                    to STATUS_VERLENGING_TIJDELIJK_TERRAS; COMMIT; """
                ).format(table_name=sql.Identifier(table_name))
            )


with DAG(
    "horeca_exploitatievergunning",
    default_args=default_args,
    description="Horeca Exploitatievergunning",
    schedule_interval="0 9 * * *",
    on_failure_callback=get_contact_point_on_failure_callback(dataset_id="horeca"),
) as dag:

    load_data = PythonOperator(
        task_id="load_data",
        python_callable=load_from_dwh,
        provide_context=True,
        op_args=[table_name_new, 4326, dag_id],
    )

    check_count = PostgresCheckOperator(
        task_id="check_count",
        sql=SQL_CHECK_COUNT,
        params={"tablename": table_name_new, "mincount": 4000},
    )

    check_geo1 = PostgresCheckOperator(
        task_id="check_geo1",
        sql=SQL_CHECK_GEO,
        params={
            "tablename": table_name_new,
            "geotype": "ST_MultiPolygon",
            "geo_column": "terrasgeometrie",
            "notnull": False,
        },
    )

    check_geo2 = PostgresCheckOperator(
        task_id="check_geo2",
        sql=SQL_CHECK_GEO,
        params={
            "tablename": table_name_new,
            "geotype": "ST_Point",
            "geo_column": "locatie",
            "notnull": False,
        },
    )

    rename_table = PostgresOnAzureOperator(task_id="rename_table", sql=SQL_TABLE_RENAME)

    # Grant database permissions
    grant_db_permissions = PostgresPermissionsOperator(task_id="grants", dag_name=dag_id)

load_data >> check_count >> check_geo1 >> check_geo2 >> rename_table >> grant_db_permissions
